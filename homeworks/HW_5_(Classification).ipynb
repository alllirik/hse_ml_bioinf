{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First of all, a few notes:\n",
        "* This work is a sequel to the previous homework assignment (HW-4 ENCODE).\n",
        "* It is assumed that in HW-4 you successeeded in data processing of ChIP-seq and ATAC-seq experiments. If you failed to do so or just didn't have time to complete HW-4, please contact me in Telegram, I will give data you can work with in this homework.\n",
        "* Comment and describe! To give you a high grade, we need to understand that you interpreted all results correctly.\n",
        "\n",
        "Remember to submit the feedback! Especially if the homework was too difficult or easy for you."
      ],
      "metadata": {
        "id": "h6rXJxY5aClz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## Target transcription factors\n",
        "\n",
        "In HW-4, you were asked to choose three ChIP-seq experiments and one ATAC-seq experiment from some cell line. Please list the names of TFs and the cell line.\n",
        "\n",
        "* Cell line:\n",
        "* TF-1:\n",
        "* TF-2:\n",
        "* TF-3:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pHJmy6k1bJm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=#c30>**[0.5]**</font> Load your data and make train/test split.\n",
        "\n",
        "Make sure that after this section, you have the following:\n",
        "* `Xtrain`/`Xtest` - table with k-mers, features for our sequences\n",
        "* `Ytrain`/`Ytest` - table with 4 one-hot-encoded columns, one for each class (background + 3 TFs)\n",
        "\n",
        "Also, some asserts for you to check if everything is fine with your dataset."
      ],
      "metadata": {
        "id": "7moazw6sBhGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data here"
      ],
      "metadata": {
        "id": "QeXcYndjBPYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert df['Class'].nunique() == 4, \"Something is wrong with the number of your classes\"\n",
        "# Ignore below if you have different maximum length of k-mers\n",
        "assert df.shape[1] == 1365, \"Do you have 1364 columns for k-mers and 1 for class?\"\n",
        "assert (df.drop(['Class'], axis=1).dtypes == 'float64').sum() == 1364, \"Some of k-mer columns are not float values\"\n",
        "assert (df['Class'].dtype == 'int64') == True, \"Your class column have non-integer values\""
      ],
      "metadata": {
        "id": "3mjdldgmGK5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Test split here"
      ],
      "metadata": {
        "id": "NI_MJ0DgGLPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training <font color=#c30>**[5]**</font>\n",
        "\n",
        "Time to train our fit-predict skills!\n",
        "\n",
        "<font color=#c30>**[0.5]**</font> Select proper target metric(s). Do you want to use micro- or macro-averaging? Justify your choice.\n",
        "\n",
        "<font color=#c30>**[1]**</font> Make sure you understand your multiclassification strategy. Briefly explain differences between One-Vs-Rest (One-Vs-All) and One-Vs-One strategies. Does every model from the list below supports multiclass classification, or do you need to wrap something into multiclass wrappers (OVR/OVO)? Check the links below for more info.\n",
        "\n",
        "  * <font color=#c30>**[ ! ]**</font> In the context of our task, we are working with **Multiclass classification**, NOT multilabel classification! (Make sure you understand the difference)\n",
        "\n",
        "  * <font color=#c30>**[ ! ]**</font> For the sake of simplicity, choose One-Vs-Rest strategy.\n",
        "\n",
        "  * Which models support multiclass classification: https://scikit-learn.org/1.5/modules/multiclass.html\n",
        "  * OVO wrapper if you need it: https://scikit-learn.org/1.5/modules/generated/sklearn.multiclass.OneVsOneClassifier.html\n",
        "  * OVR wrapper if you need it: https://scikit-learn.org/1.5/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\n",
        "\n"
      ],
      "metadata": {
        "id": "0awh0_2pqMS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer:"
      ],
      "metadata": {
        "id": "Q5LigIjQaKea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=#c30>**[3]**</font> Train and optimize hyperparameters for the following models:\n",
        "  * Logistic regression\n",
        "  * KNN\n",
        "  * Decision tree\n",
        "  * Random Forest\n",
        "  * (Optional) SVM\n",
        "  * (Optional) Any Gradient Boosting implementation\n",
        "\n",
        "  * <font color=#c30>**[ ! ]**</font> Take note that algoritmhs like SVM with nonlinear kernel will take A LOT OF TIME to train. During long calculcations, Google Colab might drop your session and you will lose your progress. To counter it, either subsample your data, make smaller grid or skip the algorithm completely (just don't delete the cell with the code and it's output).\n",
        "\n"
      ],
      "metadata": {
        "id": "r31y8cvfN_So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code...."
      ],
      "metadata": {
        "id": "40brCxzeOJOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=#c30>**[0.5]**</font> Justify the parameters grid for each model. What number of cross-validation folds did you use? Why?\n",
        "\n",
        "Your answer:"
      ],
      "metadata": {
        "id": "pfTobpeCOLIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation <font color=#c30>**[3.5]**</font>\n",
        "\n",
        "<font color=#c30>**[0.75]**</font> Use test set to rank optimized models. Describe and interpret results."
      ],
      "metadata": {
        "id": "N2I_0mdPcJ4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code...."
      ],
      "metadata": {
        "id": "BvTsdhAmcKiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color=#c30>**[0.75]**</font> For the model type of your choice, construct and interpret ROC curves (on the same figure) and calculate ROC AUC for:\n",
        "  *  each class according to One-Vs-Rest classifier scheme\n",
        "  *  micro-/macro-averaged OvR\n",
        "\n",
        "  Use this: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#plot-all-ovr-roc-curves-together"
      ],
      "metadata": {
        "id": "8FC7iEbscI43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code...."
      ],
      "metadata": {
        "id": "rq17Nwg_cLDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color=#c30>**[1]**</font> Write a custom function that plots ROC curve and calculates ROC AUC.\n",
        "\n",
        "  It should take as an input:\n",
        " * Ground-truth values for the class.\n",
        " * Probabilities predicted by the model.\n",
        "\n",
        "  Don't use sklearn in-built functions that calculate confusion matrix values and TPR/FPR.\n",
        "\n",
        " *Hint: use `numpy.trapz` to calculate AUC.*\n",
        "\n",
        "  Compare with results of sklearn implementationt.\n",
        "  "
      ],
      "metadata": {
        "id": "G5TDunIZcHVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code...."
      ],
      "metadata": {
        "id": "kzF_DgZncLp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color=#c30>**[1]**</font> Interpret features learned by each model (where applicable). Then, compare them to known TF motifs (search them online).\n"
      ],
      "metadata": {
        "id": "7QG1vsu9fqQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code...."
      ],
      "metadata": {
        "id": "0jCgY8G8aLqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra points <font color=#c30>**[1.5]**</font>\n",
        "\n",
        "In this homework, you have freedom to choose any of two tasks to earn extra points. Completing one will grant you 1.5 points. Completing both will not grant you more."
      ],
      "metadata": {
        "id": "j_GVwlcqU_O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Task 1\n",
        "\n",
        "<font color=#c30>**[1.5]**</font>  Implement your own version of the random forest algorithm, and compare results with the sklearn version using your dataset from this homework:\n",
        "\n",
        "* Write a brief explanation of how Random Forest works and how will you implement the algorithm in terms of python classes.\n",
        "* If you don't know what a python class is, I recommend you looking at some simple explanations, like this one: https://www.w3schools.com/python/python_classes.asp\n",
        "Think through:\n",
        "  * What do you need to iniate an object of RF class\n",
        "  * Which parameters and data should be stored in `self.` attributes of each class instance\n",
        "  * What inputs and outputs are for each built-in class function\n",
        "\n",
        "* Write a python class for your Random Forest algorithm. Include `__init__` method (class constructor that is called each time you create an object of class), `fit` method that trains RF according to input data and stores it inside the class instance, and `predict` method which will utilize trained trees to predict class labels for input data.\n",
        "  * You don't have to implement your own DecisionTree, use sklearn DecisionTree implementation instead.\n",
        "\n",
        "* Think of a way to utilize trained trees to learn feature importances. To do so, inside your class write a function that will aggregate importances from trained DecisionTrees, and store the final importances in `self.feature_importances` attribute.\n",
        "\n",
        "* Make sure to demonstrate the result by creating an instance of your class, fitting it with train data, predicting test data, making a barplot of feature importances, and comparing everything with sklearn implementation (using classification metrics of your choice). A code snippet for this is presented below."
      ],
      "metadata": {
        "id": "GXz_agxKb1lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExampleForest:\n",
        "  def __init__(self, ...):\n",
        "    # Parameters/Data structures are defined here\n",
        "\n",
        "  def fit(self, ...):\n",
        "    # Train DecisionTrees(from sklearn) here and save them inside class\n",
        "\n",
        "  def predict(self, ...):\n",
        "    # Make predictions using trained Trees and return result\n",
        "\n",
        "    return ..."
      ],
      "metadata": {
        "id": "cJXqilz8b3Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Showcase your class in action by making predictions on TFs dataset\n",
        "# 2) Plot feature importances stored in .feature_importances attribute of your class\n",
        "# 3) Do the same with sklearn implementation of RF (same hyperparams) and compare"
      ],
      "metadata": {
        "id": "WZ9A-NKRAATF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2\n",
        "\n",
        "If you chose this task, then it seems you don't mind going through both HW-4 and HW-5 a second time.\n",
        "\n",
        "![img](http://i0.kym-cdn.com/entries/icons/original/000/017/886/download.jpg)\n",
        "\n",
        "Now it's time to do the hard testing for your trained models! Find another tissue/cell line where we have ATAC-seq and the same set of ChIP-seq experiments available.\n",
        "\n",
        "Then you need to do the following:\n",
        "\n",
        "<font color=#c30>**[0.15]**</font>  Download ATAC-seq peaks, extract sequences and predict regions that each TF will bind.\n",
        "\n",
        "<font color=#c30>**[1]**</font>  Download ChIP-seq peaks, intersect with the ATAC-seq and compare them to the ML predictions. Next, calculate standard classification metrics (per-class and then macro averaged): $F_1$, precision, recall, accuracy. Drop regions thar overlap between multiple ChIP-seq experiments (like you did for the training).\n",
        "\n",
        "<font color=#c30>**[0.35]**</font>  Which model was the best in this scenario? Do we have any other criteria to rank models except for performance? What model would you use for real studies? Why?"
      ],
      "metadata": {
        "id": "P2EuwzDXqRff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code...."
      ],
      "metadata": {
        "id": "Zas-FjZMPdRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}