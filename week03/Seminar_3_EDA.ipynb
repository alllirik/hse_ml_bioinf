{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIRqJr5SGK88"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFsseP6Qhj6P"
      },
      "source": [
        "In this notebook we will:\n",
        "* Get to analyze life expectancy data gathere from World Health Organization\n",
        "* Learn the main steps of the Exploratory Data Analysis (EDA)\n",
        "* Discuss different techniques of data preprocessing \n",
        "\n",
        "Please leave feedback after each seminar [here](https://forms.gle/sLB8NrZZoaWgYu6F7)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_d4Bai6hepI"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Today we will work with life expectancy data gathered from the World Health Organization (WHO) website. Our goal is to perform exploratory data analysis, preprocess the dataset, and use linear regression to understand the factors associated with increased life expectancy.\n",
        "\n",
        "Original dataset is hosted on the [Kaggle](https://www.kaggle.com/kumarajarshi/life-expectancy-who) platform. To avoid the hassle of logging in, we'll use a file generously posted on GitHub by a random internet user:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO0Kb7k4GC9_",
        "outputId": "602211fe-9c22-46a4-a57f-ab29de61dd19"
      },
      "outputs": [],
      "source": [
        "# Use wget console utility to download the csv data\n",
        "!wget \"https://gist.githubusercontent.com/aishwarya8615/89d9f36fc014dea62487f7347864d16a/raw/8629d284e13976dcb13bb0b27043224b9266fffa/Life_Expectancy_Data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8yryx6ig93w",
        "outputId": "a75dcb07-8f0b-4af2-e5ef-b316270b3f26"
      },
      "outputs": [],
      "source": [
        "# Make sure that we actually downloaded Life_Expectancy_Data.csv file\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUKTWbKymqtV"
      },
      "source": [
        "# Exploratory data analysis (EDA)\n",
        "\n",
        "In short, EDA is a vaguely defined term that describes the process of “getting to know data”. Often includes preprocessing, visualization, and data summarization.\n",
        "\n",
        "We'll start from scratch and first load our dataset and get a list of the available features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbk0CbW1mrL-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the dataset (df stands for the DataFrame)\n",
        "df = pd.read_csv(\"Life_Expectancy_Data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et3HAnmRmuEs",
        "outputId": "b9f3fa58-11c9-45a8-9230-addb7d39d023"
      },
      "outputs": [],
      "source": [
        "# print the 'shape' of the matrix\n",
        "print(f\"Rows x columns: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "yo4GV3BImvj3",
        "outputId": "55ea4132-757c-4262-94de-35d3de5bd6e5"
      },
      "outputs": [],
      "source": [
        "# print the first 4 lines\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ9j323Cm3aJ"
      },
      "source": [
        "There are 23 features and 2461 entries in total. Our target variable is called `Life_expectancy`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHOgNHXGm-J4"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkIPC-lOnBw2"
      },
      "source": [
        "Always rename columns if they contain unexpected spaces, forward slashes (/) or follow awkward naming schemes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2epSwdPnCDM"
      },
      "outputs": [],
      "source": [
        "# Dictionary(mapping), old name -> new name\n",
        "mapping = {\n",
        "  'Life_expectancy ': 'Life expectancy',\n",
        "  'Adult_Mortality': 'Adult mortality',\n",
        "  'infant_deaths': 'Infant deaths',\n",
        "  'percentage_expenditure': 'Percentage expenditure',\n",
        "  'Hepatitis_B': 'Hepatitis B',\n",
        "  'Measles ': 'Measles',\n",
        "  ' BMI ': 'BMI',\n",
        "  'under_five_deaths ': 'Under-five deaths',\n",
        "  'Total_expenditure': 'Total expenditure',\n",
        "  'Diphtheria ': 'Diphtheria',\n",
        "  ' HIV/AIDS': 'HIV/AIDS',\n",
        "  ' thinness  1-19 years': 'Thinnes (1-19 years)',\n",
        "  ' thinness 5-9 years': 'Thinnes (5-9 years)',\n",
        "  'Income_composition_of_resources': 'Income composition of resources'\n",
        "}\n",
        "# Rename columns\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "X1xz8OqOnE67",
        "outputId": "8ca15a3b-2d4b-4ab4-bd5b-7b8fbb15aa90"
      },
      "outputs": [],
      "source": [
        "# Check the tail (or head)\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO_f4VAhnM34"
      },
      "source": [
        "Let's check if there are any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEeDZMOWnH7o",
        "outputId": "24cf8ab5-7560-4917-be63-0bf2069b893c"
      },
      "outputs": [],
      "source": [
        "# Special function to print inferred data types for all columns,\n",
        "# whether or not they contain null (missing) values\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6-ZQb2XnVv5"
      },
      "source": [
        "Unfortunately, the dataset is not complete — several columns have missing values. How many nulls are there in each column?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iXQFN3tnQXI",
        "outputId": "bfd0a867-380b-4197-8416-e4b5d75eb29c"
      },
      "outputs": [],
      "source": [
        "# Calculate total number of nulls for each column:\n",
        "\n",
        "# 1. isnull() returns a mask, where null values is replaced with True\n",
        "isnull = ...\n",
        "# 2. Count null records for each columns\n",
        "# (true boolean values are interpretted as 1)\n",
        "isnull = ...\n",
        "# 3. print columns with 1 or more null value\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duX9e9uWnkSh"
      },
      "source": [
        "Let's discuss the following questions:\n",
        "\n",
        "* *What can we do with null values?*\n",
        "* *Can you imagine situations where one strategy is preferred over the other one?*\n",
        "* *What strategies are preffered for large/small datasets?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmloHnAXgUZH"
      },
      "source": [
        "Let's try a few examples of how to drop null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "lL-VfH8UgJLy",
        "outputId": "0afb2a8d-905c-41e2-8944-2b94c81c183a"
      },
      "outputs": [],
      "source": [
        "df[...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "UqJJWZEtR2Va",
        "outputId": "dd6017aa-6039-4a02-b7da-6fe1902a731e"
      },
      "outputs": [],
      "source": [
        "df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVWCfXRXnhrw",
        "outputId": "75af4647-8c63-4db5-8bc1-76953363918c"
      },
      "outputs": [],
      "source": [
        "print(f\"Rows before: {df.shape[0]}\")\n",
        "\n",
        "# Drop null rows(records)\n",
        "\n",
        "# 1. Again, matrix with True in place of nulls\n",
        "isnull = ...\n",
        "# 2. Logical OR row-wise(axis=1). Are there ANY true value in each row?\n",
        "isnull = ...\n",
        "# sum true values = number of rows with nulls\n",
        "print(f\"\\tWill be dropped {...} rows\")\n",
        "# 3. Select only rows without null values\n",
        "notnull = ~... # ~ stands for element-wise NOT\n",
        "df = df[notnull]\n",
        "\n",
        "print(f\"Rows after: {df.shape[0]}\")\n",
        "\n",
        "# Same as above, but using builtin function\n",
        "# df.dropna(axis='index', how='any', inplace=True) # index = rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw7pcnfwpsry"
      },
      "outputs": [],
      "source": [
        "df = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edpmXqXon1Va"
      },
      "source": [
        "Typically, pandas can automatically detect the datatype for columns with only numeric or date records.\n",
        "\n",
        "However, you will often come across datasets in which NULL values ​​are replaced with strings such as \"Unknown\", \"NA\". Another example is categorical columns built from strings.\n",
        "\n",
        "In these cases, pandas uses the dtype `object` to efficiently store the full variety of data.\n",
        "\n",
        "Here is an example for our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hyxmiLtnm46",
        "outputId": "d4369ede-3ff1-4868-c5db-b9398f39c472"
      },
      "outputs": [],
      "source": [
        "# dtypes stands for Data Types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RANyVwOUn9oV"
      },
      "source": [
        "In most cases, you want to get rid of `object` columns by converting them to categorical or numeric dtypes. It eases the analysis and eliminates silly errors that might arise during future analysis.\n",
        "\n",
        "In practice, this is achieved by careful column-to-column analysis of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a2JPpMQn6T4",
        "outputId": "51e0a17e-7ca8-4b27-bef3-74cf08410251"
      },
      "outputs": [],
      "source": [
        "# Print all unique values\n",
        "print(\"Unique continents:\", df['Continent'].unique())\n",
        "# No unusuall entries -> cast column to categorical data type\n",
        "df['Continent'] = df['Continent'].astype('category')\n",
        "\n",
        "print(f\"New continent data type: {df['Continent'].dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBj6PxrLn_9z",
        "outputId": "48983147-7c14-4793-80bf-d402f0a50c11"
      },
      "outputs": [],
      "source": [
        "# Similarly, cast Country and Status to categorical\n",
        "for x in 'Status', 'Country':\n",
        "  print(f\"{x} values: {df[x].unique()}\\n\")\n",
        "  df[x] = df[x].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxynC3LnoCV5",
        "outputId": "13ff62be-c112-40df-ab7d-23549a127b6f"
      },
      "outputs": [],
      "source": [
        "# The new categorical columns include a .cat property with a .categories nested\n",
        "# property that lists all the available categories for the column:\n",
        "print(\"Countries: \", df['Country'].cat.categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmwDsEhJqje-"
      },
      "source": [
        "Another useful function for dealing with unknown data is `df[column].value_counts()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xdfmH6Zq56v",
        "outputId": "f321d78c-2e38-4507-c020-7b3a9d70348d"
      },
      "outputs": [],
      "source": [
        "df['Population'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxPQect0qgQk"
      },
      "outputs": [],
      "source": [
        "assert df['Population'].isna().sum() == 0\n",
        "# Skip records with unknown population\n",
        "mask = df['Population'] != 'Unknown'\n",
        "df = df[mask].copy()\n",
        "\n",
        "# Cast column to float data type\n",
        "df['Population'] = df['Population'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnNZLPlHt1YW"
      },
      "outputs": [],
      "source": [
        "assert df['Diphtheria'].isna().sum() == 0\n",
        "df['Diphtheria'] = df['Diphtheria'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFXZDwD6tuRz"
      },
      "source": [
        "Make sure we no longer have `object` columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqOCSyMlrKQL",
        "outputId": "c1df8057-6185-4bea-e6cd-a25ab233ed7d"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZatsD7JSuTYD"
      },
      "source": [
        "That's all for now! But is that all we need before moving on to visualization?...\n",
        "\n",
        "In fact, the following table is a key part of \"preprocessing\" in a broad sense.\n",
        "You **must** understand your data, know what the numeric values ​​in each column mean, before going any further.\n",
        "\n",
        "|Field|Description|\n",
        "|---:|:---|\n",
        "|Life expectancy|Life Expectancy in age|\n",
        "|Adult Mortality|Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)|\n",
        "|infant deaths|Number of Infant Deaths per 1000 population|\n",
        "|Alcohol|Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)|\n",
        "|Percentage expenditure|Expenditure on health as a percene of Gross Domestic Product per capita(%)|\n",
        "|Hepatitis B|Hepatitis B (HepB) immunization coverage among 1-year-olds (%)|\n",
        "|Measles|Measles - number of reported cases per 1000 population|\n",
        "|BMI|Average Body Mass Index of entire population|\n",
        "|Under-five deaths|Number of under-five deaths per 1000 population|\n",
        "|Polio|Polio (Pol3) immunization coverage among 1-year-olds (%)|\n",
        "|Total expenditure|General government expenditure on health as a percene of total government expenditure (%)|\n",
        "|Diphtheria|Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)|\n",
        "|HIV/AIDS|Deaths per 1 000 live births HIV/AIDS (0-4 years)|\n",
        "|GDP|Gross Domestic Product per capita (in USD)|\n",
        "|Population|Population of the country|\n",
        "|Thinness (1-19 years)|Prevalence of thinness among children and adolescents for Age 10 to 19 (%)|\n",
        "|Thinness (5-9 years)|Prevalence of thinness among children for Age 5 to 9(%)|\n",
        "|Income composition of resources|Income composition of resources|\n",
        "|Schooling|Number of years of Schooling(years)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl8UJgbfuhzv"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPZo5ufR3CUF"
      },
      "source": [
        "We will start our visualization with simple box plots, which usually have the following notation:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://previews.dropbox.com/p/thumb/ACCcevd-ZKxEWIWALqW4Y2oSG0-lv5e925hceCNr6eH7x8tG1-HbscaJ7CRTpHCr_CvlZhXeKTPcdgn-umlmKpGCCGEGOLfcp6Sph7lm5BjBs7Ap7znhFNI6u4j4C6dtjKBP2IadVHuhjcD4ONc-CTnF_vHiaKitor4ewIK5rkvK7JWJmrXdp_7RhTB9jKQD9fX_MiBOnt3xYHmtpYi8Ntr7tI-jag-ZhCIEaRnsqyQul55kZ7lxXqVHFk1E9CO6LqDOxkF7I6qT1fRE5clm8doZLrQltGHyIpU119Ps5qrGRfCSeCDDW494NRP2qIr3l2PFgnC0iro4wSG98Zxq8-Jb/p.png\" width=550>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "d8ELaZEYuEox",
        "outputId": "3eba276e-6aae-4992-ab68-6462ca296dfd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get numeric columns only\n",
        "numeric = list(df.select_dtypes(include=np.number).columns)\n",
        "\n",
        "# Create figure with 20 subplots\n",
        "fig, axes = plt.subplots(..., figsize=(26, 14))\n",
        "# By default, axes is a 4x5 matrix, ravel it to a flat array with 20 elements\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Sanity check\n",
        "assert len(numeric) == len(axes)\n",
        "\n",
        "# Plot boxplot for each numeric column\n",
        "for col, ax in zip(numeric, axes):\n",
        "  sns.boxplot(y = df[col], ax=ax)\n",
        "  #sns.violinplot(y = df[col], ax=ax)\n",
        "  ax.set_title(col)\n",
        "  ax.set(xlabel='', ylabel='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyzYkIPi3MFL"
      },
      "source": [
        "`describe` method is a nice complementary way to better understand the composition of each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "Ktqf7VkD3EYm",
        "outputId": "29d47567-14a8-4098-e6da-a078bd89b9f8"
      },
      "outputs": [],
      "source": [
        "# Perform `describe` only for numeric columns\n",
        "df[numeric].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GomEaZdb3Qgx"
      },
      "source": [
        "We can also create a histogram for each column using the built-in pandas method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "vnHIweDT3OWP",
        "outputId": "af068ac0-2c3b-4bbb-848d-078e8718dd66"
      },
      "outputs": [],
      "source": [
        "# `_ =` construct to ignore return data from the hist method\n",
        "_ = df[numeric].hist(figsize=(22, 18), bins=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNIbZcaO3WHi"
      },
      "source": [
        "Discussion time!\n",
        "\n",
        "* *Do we have columns that look extremely suspicious in general?*\n",
        "* *Are there \"special\" values ​​in any of the numeric columns?*\n",
        "* *What can we do with the outliers?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx25CeYP3Znz"
      },
      "source": [
        "One way to deal with outliers is to \"trim\" the data and keep only records where values in each column are within [$\\alpha$, 1-$\\alpha$] percentiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf2DW2VY3R6Z",
        "outputId": "fabfd43d-0905-4025-a75d-ed263326fa47"
      },
      "outputs": [],
      "source": [
        "print(f\"Records before: {len(df)}\")\n",
        "\n",
        "# Get 0.01 and 0.99 percentile for each column, alpha = 0.01\n",
        "low, high = 0.01, 0.99\n",
        "quantiles = df[numeric].quantile([low, high])\n",
        "\n",
        "# Remove Year column from the 'trimming' procedure\n",
        "quantiles.drop(columns=['Year'], inplace=True)\n",
        "\n",
        "for col in quantiles.columns:\n",
        "  low, high = quantiles[col].tolist()\n",
        "  mask = (...) & (...)\n",
        "  df = df[mask]\n",
        "\n",
        "print(f\"Records after: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "X95Eplvj3VYm",
        "outputId": "a2d35e51-67ff-4173-d08f-9d9cc02f1fb8"
      },
      "outputs": [],
      "source": [
        "quantiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "Ix-rcv3B3dL1",
        "outputId": "121fcd74-cea1-4db2-c05e-b94147719bb7"
      },
      "outputs": [],
      "source": [
        "_ = df[numeric].hist(figsize=(22, 18), bins=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92lB4Cpg3oJ6"
      },
      "source": [
        "So, we dropped about 300 records, but there is still a suspicious peak at '0' for the income composition of resources...\n",
        "\n",
        "*Why?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMuj02we3iHY",
        "outputId": "d6562c76-baec-4896-c5ef-bd13cfb3b7d0"
      },
      "outputs": [],
      "source": [
        "print(f\"Recods before: {len(df)}\")\n",
        "df = df[df['Income composition of resources'] > 1e-6].copy()\n",
        "print(f\"Recods after: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "-lnJW36PZ8kC",
        "outputId": "4c54afb5-1684-4db9-bf81-068e42a2b5a8"
      },
      "outputs": [],
      "source": [
        "df['Income composition of resources'].hist(bins=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j93kJQ035VV"
      },
      "source": [
        "Okay, all the fuss is interesting, but let's get down to business. We are (presumably) paid to interpret, not clean up the data(even if it is a part of the process).\n",
        "\n",
        "Building a correlation heatmap is a good place to start:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "LR-LSbil33Uw",
        "outputId": "08421c35-85eb-4c9d-f8dd-570b8886931e"
      },
      "outputs": [],
      "source": [
        "correlations = df[numeric].corr(method='spearman') # try pearson\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "sns.heatmap(correlations, square=True, annot=True, linewidths=0.25)\n",
        "plt.title(\"Correlation matrix for numeric features\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8p_ydGO3_Fw"
      },
      "source": [
        "* *What expected / unexpected correlations can one find here?*\n",
        "* *Can you explain them with common sense?*\n",
        "\n",
        "Never forget that correlation does not imply causation. [Here](https://www.tylervigen.com/spurious-correlations) are a few funny examples.\n",
        "\n",
        "Can you explain them with common sense?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlEOIQFr4PT2"
      },
      "source": [
        "To keep you busy a little longer, here is the relationship between longevity and each feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "rXshKKJ138F_",
        "outputId": "95ccdd86-cde4-40e4-d63a-f116810ee714"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(26, 14))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for ax, col in zip(axes, numeric):\n",
        "  ax.scatter(df[col], df['Life expectancy'])\n",
        "  ax.set_title(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxRv4B0c4WpY"
      },
      "source": [
        "What does it take to confidently argue that overall life expectancy increased from 2000 to 2015?\n",
        "\n",
        "In other words, imagine that you have to defend this point of view during a WHO meeting. What tools/graphs would you use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHbq0WiI4aMZ"
      },
      "source": [
        "The game, find your country, has begun:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "9X2or46B4Tzm",
        "outputId": "ca27470a-4b60-4135-ad4b-6943fdf702fc"
      },
      "outputs": [],
      "source": [
        "countries = df.groupby('Country')['Life expectancy'].mean()\n",
        "\n",
        "ax = countries.plot(kind='bar', figsize=(50,15), fontsize=25)\n",
        "ax.set_title(\"Life_Expectancy w.r.t Country\",fontsize=40)\n",
        "ax.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yflGv6PD4gPt"
      },
      "source": [
        "Empty columns indicate categories that were present in the original dataset, but disappeared after filtering.\n",
        "\n",
        "Let's remove these \"empty\" categories from the general list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO1zi7MZ4d0p"
      },
      "outputs": [],
      "source": [
        "# select only categorical columns\n",
        "categorical = df.select_dtypes('category').columns.tolist()\n",
        "\n",
        "for col in categorical:\n",
        "  # remove unused categories inplace(!)\n",
        "  df[col].cat.remove_unused_categories()\n",
        "  # how to run this without inplace?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* How do we perform EDA if given dataset has too many features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qS-d1TW5BTx"
      },
      "source": [
        "# ML-specific preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2GnKI_9GuWW"
      },
      "source": [
        "## Dummy variables (one-hot encoding)\n",
        "\n",
        "Dummy variables are a way to encode categorical data in numeric format.\n",
        "\n",
        "This is commonly used for machine learning since fundamental math models cannot handle strings / categories by design (in most cases)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1PwtP8XHe61"
      },
      "source": [
        "Overall, we have 20 columns with 1 target variable.\n",
        "Among them, we will drop 'Country', 'Continent' and 'Year' features. *Why?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLC4PM0xlnlp"
      },
      "outputs": [],
      "source": [
        "# Glue list with columns\n",
        "columns = numeric + categorical\n",
        "# Drop columns in-place\n",
        "for x in \"Country\", \"Continent\", \"Year\":\n",
        "  columns.remove(x)\n",
        "# Select only remaining columns from the data frame\n",
        "df = df[columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZnkIv8MlP3A"
      },
      "outputs": [],
      "source": [
        "# Replace column Status with dummy variables\n",
        "ddf = pd.get_dummies(df, columns=['Status'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "KsqQmUS0lNjX",
        "outputId": "393b4b12-a551-4a93-e55b-84a59f6455c3"
      },
      "outputs": [],
      "source": [
        "ddf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs84bNkOIEy1"
      },
      "source": [
        "Dummy variables (also known as hot-coded functions) are constructed from categorical data as follows:\n",
        "1. Add N new columns, where N is the number of categories.\n",
        "2. For each entry, set 0 for all N categories except one corresponding to the current row category.\n",
        "\n",
        "\n",
        "*How would you one-hot encode DNA sequences?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1dmlCVUl3wz"
      },
      "source": [
        "## Train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1fw5wG9I6W3"
      },
      "source": [
        "*Why random_state is fixed in the next block of code?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QsEITNdluxP",
        "outputId": "07f5fa5d-0382-448c-c887-ec8ca50cd2d0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = ddf.pop(\"Life expectancy\")\n",
        "Y = Y.values\n",
        "X = ddf.values\n",
        "\n",
        "print(\"Before:\")\n",
        "print(f\"\\tX: {X.shape}; Y: {Y.shape}\")\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state = 42\n",
        ")\n",
        "\n",
        "print(\"Train:\")\n",
        "print(f\"\\tX: {X_train.shape}; Y: {Y_train.shape}\")\n",
        "print(\"Test:\")\n",
        "print(f\"\\tX: {X_test.shape}; Y: {Y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Transformation\n",
        "\n",
        "* Machine learning models make a lot of assumptions about the data\n",
        "\n",
        "* In reality, these assumptions are often violated\n",
        "\n",
        "* We build pipelines that transform the data before feeding it to the learners\n",
        "\n",
        "    * Scaling (or other numeric transformations)\n",
        "\n",
        "    * Encoding (convert categorical features into numerical ones)\n",
        "\n",
        "    * Automatic feature selection\n",
        "\n",
        "    * Feature engineering (e.g. binning, polynomial features,…)\n",
        "\n",
        "    * Handling missing data\n",
        "\n",
        "    * Handling imbalanced data\n",
        "\n",
        "    * Dimensionality reduction (e.g. PCA)\n",
        "\n",
        "    * Learned embeddings (e.g. for text)\n",
        "\n",
        "* Seek the best combinations of transformations and learning methods\n",
        "\n",
        "    * Often done empirically, using cross-validation\n",
        "\n",
        "    * Make sure that there is no data leakage during this process!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout [this page](https://ml-course.github.io/master/notebooks/06%20-%20Data%20Preprocessing.html#lecture-6-data-preprocessing) for more info and examples for data preprocessing tricks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJel8wmLZD39"
      },
      "source": [
        "### Normalization\n",
        "\n",
        "Most machine learning algorithms perform better with normalized data, i.e. ***approximately*** normal. For example, this is important for PCA and gradient methods, but not for decision trees.\n",
        "\n",
        "The only trick here is that the normalization parameters should be determined only on the training set. We also don't really care about categorical columns, they will be processed together with numerical columns.\n",
        "\n",
        "* *Why?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Makes sure that feature values of each point (each row) sum up to 1 (L1 norm)\n",
        "\n",
        "    * Useful for count data (e.g. word counts in documents)\n",
        "\n",
        "* Can also be used with L2 norm (sum of squares is 1)\n",
        "\n",
        "    * Useful when computing distances in high dimensions\n",
        "\n",
        "    * Normalized Euclidean distance is equivalent to cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "algExuVSni2l"
      },
      "source": [
        "### Power transform\n",
        "\n",
        "The first step is to remove skewness for features like measles and population. We will do this by a so-called [power transform](https://en.wikipedia.org/wiki/Power_transform).\n",
        "\n",
        "Another option is the logarithmic transformation (e.g. $\\hat{x} = log(x + 1)$), which is especially popular in bioinformatics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVjuyel3pU5q",
        "outputId": "a53e1be1-dc11-4662-b234-618338527f86"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "ptransform = PowerTransformer(standardize=False).fit(X_train)\n",
        "print(\"Estimated lambda: \", ptransform.lambdas_)\n",
        "\n",
        "X_train = ptransform.transform(X_train)\n",
        "X_test = ptransform.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBvPF0Z8nf6v"
      },
      "source": [
        "It's ok to power transform all features, even almost normal. It usually won't hurt, but be sure to do a visual sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "VUSDH_3EqoJD",
        "outputId": "24baa587-1ad9-4d50-d72c-74b379d9bbd5"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(26, 14))\n",
        "axes = axes.ravel()\n",
        "\n",
        "X, Y = X_test, X_train\n",
        "for ax, col in zip(axes, range(X.shape[1])):\n",
        "  sns.histplot(x=X[:, col], ax=ax)\n",
        "  ax.set_title(ddf.columns[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB1dtDvBthui"
      },
      "source": [
        "As expected, ~magic is forbidden outside of Hogwarts~ some variables are only approximately normal and there is little we can do about it.\n",
        "\n",
        "The harsh reality is that there are many distributions that are far from normal or even lognormal. And that's okay, in most cases our algorithms are robust enough to handle this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOwQ-c5TvUot"
      },
      "source": [
        "### Standardization\n",
        "\n",
        "The second step is to bring the data to a similar scale. This is routinely done by a standartization, i.e. mean removal and variance scaling.\n",
        "\n",
        "$$\\hat{X} = \\frac{X - E[X]}{\\sigma}$$\n",
        "\n",
        "After this transformation, the data should have zero mean and unit variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZUEPOkAZjxW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMm_wy1oaYjt"
      },
      "source": [
        "Check a random column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41L-rhTGaEAm",
        "outputId": "28e7f823-337d-4866-8601-4a0c538fc481"
      },
      "outputs": [],
      "source": [
        "colind = 0\n",
        "print(f\"Train: {X_train[:, colind].mean():.3f}, {X_train[:, colind].std():.3f}\")\n",
        "print(f\"Test: {X_test[:, colind].mean():.3f}, {X_test[:, colind].std():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSjspd6VwcYQ"
      },
      "source": [
        "Visual check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "OOFMPrUcwdU5",
        "outputId": "0e27acc5-5f33-4e8e-dc6e-12ffc5e86539"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(26, 14))\n",
        "axes = axes.ravel()\n",
        "\n",
        "X = X_test\n",
        "for ax, col in zip(axes, range(X.shape[1])):\n",
        "  sns.histplot(x=X[:, col], ax=ax)\n",
        "  ax.set_title(ddf.columns[col])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
